%\documentclass[a4paper,11pt]{article}
\documentclass[a4paper]{IEEEtran}
\usepackage[pdftex]{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{cite}
\usepackage{array}
\usepackage{algorithm2e}
\usepackage{url}
\input{notation.inc}
\title{Binary Matrix Factorization}\author{Ignacio Ram\'{i}rez}
%
%
%
\begin{document}
%
\maketitle
%
\begin{abstract}
Matrix factorization is a key tool in data analysis; its applications include recommender systems, correlation analysis, signal processing, among others. There are several ways to factorize a matrix: notorious examples include PCA and its robust versions. Binary matrices are of particular interest as they arise naturally in many applications, can be stored   efficiently, and suitable algebras exist for combining their entries; this has led to the development  of a number of techniques which are specific to this case; most of them for the task of data mining.
We propose a binary matrix factorization method which combines tools from signal processing  and data mining. All operations are binary and carried out using bitwise operators. thus avoiding costly floating point operation and enabling the use of vectorized instructions. Furthermore, the algorithm is shown to converge to a local minimum in a finite number of steps,  thus performing no worse than similar real-valued optimization methods. Finally, our preliminary results show that the proposed method is effective at producing interpretable factorizations of various data types of interest.
\end{abstract}%
\begin{IEEEkeywords}
binary matrix factorization, binary dictionary learning, data mining
\end{IEEEkeywords}
%
\section{introduction}

We consider the problem of approximating a binary matrix $X \in \{0,1\}^{m{\times}n}$ as the product of two other binary matrices $U \in \{0,1\}^{m{\times}k}$ and $V \in \{0,1\}^{n{\times}p}$ plus a third \emph{residual} matrix $E$,
\begin{equation}
X = UV\transp + E.
\label{eq:mf}
\end{equation}
The problem \refeq{eq:mf} has been addressed using various methods from different fields for over a hundred years. A famous example is Principal Component Analysis (PCA), pioneered by Pearson~\cite{pearson01}. The PCA method is based on the Singular Value Decomposition (SVD) of a matrix, $X=U{\Sigma}V\transp$ where $U$ and $V$ are matrices of $p=\min\{m,k\}$  orthogonal columns each and $\Sigma$ is a diagonal $p{\times}p$ matrix whose diagonal elements $\sigma_{11} \geq \sigma_{22} \geq \ldots \geq \sigma_{pp} \geq 0$, are the so-called \emph{singular values} of $X$. The PCA method decomposes $X$ by selecting the first $p' \leq p$ columns of $U$, $V$, and upper-left $p'{\times}p'$ submatrix of $\Sigma$ to form an approximation of $X$, $X'=U'\Sigma'(V')\transp$. It can be shown that $X'$ is the rank-$p'$ matrix that best approximates $X$ in Frobenius norm.

Dozens of variants, with different properties, have been developed since PCA was first proposed. Some methods target the robustness of PCA against perturbations in the entries of $X$. These are generally called Robust PCA methods; see~\cite{rpca-review} for a review. 

Other methods impose additional restrictions on the factors; a popular one is to enforce $U$ and $V$ in \refeq{eq:mf} to be non-negative, resulting in the so called Nonnegative Matrix Factorization (NMF) family of methods~\cite{nmf-review}. The restriction of non-negativity is often imposed to improve interpretability  or because the factors are expected to be non-negative by the nature of the problem to be solved (e.g., probabilities). For similar reasons, in different contexts, some matrices cannot be reasonably assumed to be real numbers. If the matrix entries should be integer (or naturals), the usual approach is to treat them as real (or non-negative) numbers anyway and then truncate the result. In other situations, only a relative order between the values can be established. A  more complicated setting is when the entries are just categories or labels (such as Football Teams) for which there is no reasonable relationship  but ``equal'' or ``different''. We call such matrices \emph{categorical}. 

Binary matrices, which admit only two possible values for their entries (say $0$ and $1$ to fix concepts), are an interesting type which can be considered both categorical and numeric at the same time: their values may be considered labels, but can also be considered numbers and an algebra may be constructed to operate on them, vector spaces can be defined with its entries, etc. Perhaps because of this, most existing \emph{binary matrix factorization} (BMF) methods still rely on real-valued matrix factorization methods~\cite{bmf-prev}; for example, constraints can be added so that the factors are within the $[0,1]$ range. An important exception, derived for the task of large scale data mining, is the Proximus~\cite{proximus} method, an extremely fast method which relies on binary-only operations; in fact, Proximus is one of the pillars of the present work.

Finally, there are methods which are in fact instances of MF although they were not conceived to be so. The second pillar of our proposed solution is inspired by the \emph{dictionary learning} techniques whose origins date back to the 2000's~\cite{olshausen97,engan00,elad06} within the field of signal processing

This work proposes a novel method for binary matrix factorization which combines ideas and tools from dictionary learning and data mining methods while relying entirely on binary algebras which are in turn implemented using the standard bit-wise operators available in any microprocessor, plus the vectorized operations available in most modern processors~\footnote{Of particular impact is the recent addition of the single-instruction \texttt{popcount}~\cite{popcount} operation to Intel processors, which makes summation and inner products even more efficient.}. The result is an extremely fast and scalable method, specially when compared to its real-relaxation counterparts. 

Despite the restriction to binary operations, the convergence guarantees offered by our method are no worse than those of most matrix factorization methods (with the exception of PCA, which can be solved exactly) and, in particular, dictionary learning;  as most MF problems are non-convex, the solutions can be guaranteed to be locally optimal at best, and this carries out to our case as well. Furthermore, being of discrete nature, our method is guaranteed to converge in a finite number of steps. This is also a crucial advantage over generic MF methods in terms of speed and accuracy.

%The problem \refeq{eq:mf} is generally non-convex. There are very special cases in which it can be solved exactly, or it reduces to a tractable convex problem under particular conditions. Most DL approaches fall into the general setting where \refeq{eq:mf} is NP-hard and only local convergence (to a stationary point) can be guaranteed. This is usually achieved through \emph{alternate (block) minimization} on $D$ and $A$,

Finally, we show that the results obtained with our method are clearly interpretable in a variety of cases; we also show how to apply them to perform standard restoration tasks such as classification and correcting noisy or corrupt datum (denoising).

The rest of this document is organized as follows: Section~\ref{sec:dictionary-learning} is devoted to describing the basic ideas and methods in dictionary learning. Section~\ref{sec:proximus} describes the Proximus method, from which some key steps of our algorithm and its properties are derived. Section~\ref{sec:b-svd} describes our proposed method, its properties, discusses some interesting variants and proposes efficient strategies for dealing with the problem of model selection (which in in this case amounts to choosing the rank of the factors). Section~\ref{sec:applications} showcases how to apply the result of our method to the problems of classification and denoising.
We present and discuss the results obtained on those problems in Section~\ref{sec:results}, and provide concluding remarks in Section~\ref{sec:conclusion}.

\section{Dictionary Learning and sparsity}

The aim of \emph{Dictionary Learning} methods is to obtain efficient representations of specific types of signals in terms of basis or \emph{frames}~\cite{frame-review} which are adapted to the particularities of the signal type at hand. This is in contrast to the traditional approach where fixed transforms such as the Discrete Fourier Transform (DFT) or any of the wide family of Discrete Wavelet Transforms (DWT) are used to compactly represent the salient data of signals such as audio tracks or natural images. In this setting, the columns of $X$ are data samples, for example short audio segments or pixel values from patches of natural images. The dimension $m$ of the data samples is usually much smaller than the number of samples $n$. In the more general context of matrix factorizations, we say that these are extremely \emph{fat matrices}; this is an important aspect to consider in MF algorithms, as we will see later.
For sufficiently large $n$, we can tailor a \emph{dictionary} $D \in \reals{m{\times}p}$ such that each sample in $X$ can be compactly represented in terms of a few columns of $D$; by compact we mean that we can represent $X=DA+E$ with $\|E_i\| \ll \|X_i\|$ and $\|A_i\|_0 \ll p$ for most values of $i$; here $\|A_i\|_0$ is the pseudo-norm which counts the non-zeros of the vector $A_i$, in other words, the Hamming weight of $A_i$. 

As we can see from the previous discussion, contrary to the more general MF approach where the factors $U$ and $V$ have similar roles, those of $D$ and $A$ are quite different. The columns of the matrix $D$ are called ``atoms'' and are supposed to embody typical patterns observed throughout the columns of $X$, while the columns of $A$ specify the linear combination of columns of $D$ that better approximates the corresponding columns of $X$.

The typical, basic approach to Dictionary Learning is to obtain a local solution by alternate minimization in $D$ and $A$,

\begin{eqnarray}
A\iter{k+1} =& \arg\min_{A} \{ f(D\iter{k}-A) + g(A) \} \\
D\iter{k+1} =& \arg\min_{D} \{ f(D-A\iter{k+1}) + g(A\iter{k+1}) \},
\label{eq:dl}
\end{eqnarray}

where $f(\cdot)$ and $g(\cdot)$ are \emph{fitting} and \emph{regularization} functions respectively. We now briefly discuss two popular Dictionary Learning methods; most of the techniques developed later on (see~\ref{dl-review}) can be seen as modifications of these two.

\subsection{MOD} For the case $f(D,A)=\|DA-X\|_2^2$ and $g(A)=\sum_{i}\|A_i\|_1$ the \emph{Method of Directions} (MOD)~\cite{mod}, is given by
\begin{eqnarray}
A_j\iter{k+1}\!\! &=&\!\! \arg\min_{a \in \reals^p} \{ \|x_j - D\iter{k}a \|_2^2 + \|a\|_1, \} \\
D_r\iter{k+1}\!\! &=&\!\! u_r/\min\{1,\|u_r\|_2\},\\
\!\!&&\!\!u_r = X(A\iter{k+1})_r\transp\left(A\iter{k+1}(A\iter{k+1})\transp\right)^{-1}\!\!\!,\,
\label{eq:mod}
\end{eqnarray}
where $A_j$ and $D_r$ are the $j$-th and $r$-th columns of $A$ and $D$ respectively. The first step corresponds to an $\ell_1$-regularized least squares regression problem on each column of $A$, also known as LASSO~\cite{lasso}.
In the second step, each atom $D_r$ of the dictionary corresponds to a normalized down version of the least squares solution $u_r$. Here too, (although ot ``officially'' part of the algorithm,) it is customary  to apply some sort of regularization so that $AA\transp$ is invertible and well conditioned.

\subsection{K-SVD} 

In this case, $f(\cdot)$ is again the squared $\ell_2$ norm and $g(\cdot)$ corresponds to the $\ell_0$ pseudo-norm, the Hamming weight. 

\paragraph{Coefficients update} The columns of $A$ are computed using a greedy method known as OMP (Orthogonal Matching Pursuit)~\cite{omp}, which   under certain conditions can be shown to provide the actual solution to the $\ell_0$ corresponding penalized least squares problem (see~\cite{tropp04?}). A simpler variant of this step uses the (non-orthogonal) Matching Pursuit (MP)~\cite{mp}, which is given by, 
\begin{algorithm}[ht]
\KwData{vector to encode $x$, dictionary $D$, maximum residual norm $\epsilon$}
\KwResult{Optimal coefficients for $x$, $a$}
Set iteration $k=0$, residual $r\iter{0}=x$, coefficients $a\iter{0}=0$\;
\While{$\norm{r\iter{k}} \geq \epsilon$}{
  $i = \arg\max \left\{ D_i\transp r\iter{k} \right\} $ \;
  $a_i \leftarrow D_i\transp r\iter{k}$ \;
  $r\iter{k+1} \leftarrow r\iter{k} - a_iD_i $\; 
  $k \leftarrow k+1 $ \;
}
\label{alg:omp}
\end{algorithm}
What MP does at each iteration is to project the residual onto the atom that is most correlated to it, and then remove the projection from the residual. For this to work well, the atoms must be normalized to have $\ell_2$ norm 1.

\paragraph{Dictionary update} The second stage, instead of being a block descent on $D$, performs a rank-one update of each atom $D_r$ and the sub-row formed by the non-zero entries of the row $A^r$, which we call $A^r_{nz}$. Let $R = X - D\iter{k}A\iter{k} + D\iter{k}_r(A\iter{k})^r,$ be the residual matrix obtained by removing the contribution of $D\iter{k}_r$ from the current approximation of $X$. The values of $D_r$ and $A_{nz}^r$ are then replaced by the first pair of left and right eigenvectors of the SVD decomposition of $R$, 
\begin{equation}
D_r = U_1,\quad A_{nz}^r=V_1,\quad U\Sigma V\transp=R.
\label{eq:ksvd3}
\end{equation}
The above procedure is performed sequentially for each atom, from $r=1$ to $r=p$. Note that, since $A$ is updated as well, more passes could be performed in this fashion. K-SVD however performs this only once, thus finishing the alternate minimization iteration.

The K-SVD dictionary step is significantly more costly than that of MOD, but usually requires significantly less iterations to converge to a good result. MOD, is better suited for online dictionary adaptation, as fast approximations of the statistics $AA\transp$ (which can be thought of as the Hessian associated to minimizing $D$)  and $XA\transp$ can be efficiently updated on a sample to sample basis.


\section{Binary Matrix Factorization and Proximus}

Several methods have been proposed for the BMF task. Some early ones include SDD~\cite{sdd}, where $U$ and $V$ are the sign $\{-1,0,+1\}$ of the corresponding matrices in the SVD decomposition of $X$. Although SDD is ternary, it forms the basis for the Proximus~\cite{proximus} algorithm which in turn is the basis of one of our proposed methods, and serves as a simple example of adapting a real-valued method (which for very large datasets can be very slow to compute) to the problem of discrete matrix approximation.
More recent works include the BMF algorithm of~\cite{zhang07} which is based also on a real-valued non-negative matrix factorization optimization method.
(The work ~\cite{zhang07} discusses other proposed methods as well, which we will not describe here for lack of space). % son BiMax, ISA, SAMBA and BND, de los Proximus guys

\def\indicator{\mathbf{1}}
Let the indicator function $\indicator(\cdot)$ be defined so that $\indicator(cond)=1$ if $cond$ is true, and $0$ otherwise. Also, let $a \oplus b$ denote modulo-2 addition, also known as eXclusive OR (XOR). The basic Proximus algorithm~\cite{proximus} provides an approximation to the rank-one matrix $uv\transp$ that is closest to the given matrix $X$ in Hamming distance,
\def\bool{\mathrm{bool}}
\begin{algorithm}
\KwData{matrix $X \in \{0,1\}^{m{\times}n}$, $u\iter{0} \in \{0,1\}^m$, $v\iter{0} \in \{0,1\}^n$ }
\KwResult{Vectors $u$, $v$ so that $X \approx uv\transp$}
Set iteration $k=0$\;
\Repeat {$u\iter{k}(v\iter{k})\transp = u\iter{k-1}(v\iter{k-1})\transp$} {
  $u\iter{k+1}_i\!\! \leftarrow \indicator \left( A^i v\iter{k} > \|v\iter{k}\|_0/2 \right),\;i=1,\ldots,m$ \;
  $v\iter{k+1}_j\!\! \leftarrow \indicator\left(A_j\transp{u\iter{k+1}}\!>\!\|u\iter{k+1}\|_0/2 \right),j=1,\ldots,n$ \;
  $k \leftarrow k+1 $ \;
}
\label{alg:proximus}
\end{algorithm}

\begin{proposition}
The output $(u,v)$ of the Proximus Algorithm~\ref{alg:proximus} is a local optimum of the problem $\min \|X - uv\transp\|_0$. 
\end{proposition}
\begin{proof}
 Given $v\iter{k}$, it is easy to check that the update $u\iter{k+1}$ in Algorithm~\ref{alg:proximus} is the value of $u$ that \emph{globally} minimizes $\|X \oplus uv\iter{k+1} \|$ (if $ s\iter{k}_i = w\iter{k}/2$, both $0$ and $1$ are equally optima; in such case, we default to $0$). The same happens with the update $v\iter{k+1}$ given $u\iter{k+1}$. Therefore, $h\iter{k}=\|X \oplus u\iter{k}(v\iter{k})\transp \|_0$ cannot increase with $k$. As $h\iter{k} \geq 0$ is bounded, non-increasing, and the iterates can take on a finite number of values, the sequence $h\iter{k}$ must converge  after a finite number of steps. Let $(u,v)$ be the arguments at which the stopping condition is satisfied. By definition of the algorithm, no change in $u$ or $v$ decreases the objective. This guarantees that $(u,v)$ is a local minimum in a Hamming ball of radius at least $1$.\footnote{notice that we cannot guarantee that a simultaneous change in a single coordinate of $u$ and a single coordinate of $v$ will not decreasae the cost function!.} 
\end{proof}

\subsection{Recursive Proximus}

The authors of~\cite{proximus} also propose a method for further analyzing $X$. Although both are treated as equals by the Proximus algorithm, the authors assign different roles to $u$ and $v$. The former is called the \emph{presence} vector, and the latter is called the \emph{patter} vector. In other words, the interpretation given is that $v$ represents a dominant pattern in $X$ and a $1$ in $u$ indicates a row in which that pattern is present. With this rationale in mind, the authors propose to split $X$ in two matrices: one, $X_1$, in which $v$ is present, and another, $X_1$, in which it is not. As the rank-one approximation $(u,v)$ so far does not explain $X_0$, the Proximus algorithm is applied again on $X_0$ in order to find a new rank-one model that does. 
FALTA: detalle de como se tratan

Another possibility, not discussed in the paper, is to split the matrix in \emph{four}: the submatrices corresponding to indexes for which $(u_i,v_j)$ are $(0,0)$, $(0,1)$, $(1,0)$ and $(1,1)$. This would allow for a rank-k approximation of $X$ to be computed recursively using Proximus as well. We call this Q-Proximus (Q for \emph{quad}).

\subsection{Initialization}

Initialization is always a serious issue in non-convex problems, particularly in dictionary learning methods. The binary case is no exception. The authors of Proximus propose a number of initialization methods, namely: 

%\paragraph{partition} Works only when $K \leq M$. This implementation ranks the   dimensions of the data matrix $X$ (that is, the number of columns of $X$) in descending %weight order. Then, each atom  $k$ is initialized as the Hamming average of all samples in $X$ which have a value of $1$ in the $k$-th  ranked dimension.
\paragraph{partition} The authors propose to choose one \emph{separator} column (they do not specify how) and then use that column, along with the Hamming centroid of the rows for which the corresponding value in the separator column is $1$.
\paragraph{greedy graph growing} Let $P$ be a subset of rows whose only initial element is a randomly select row. Then, recursively add all rows which share a non-zero location with \emph{any} row currently in $P$ until no additional rows can be added. The initial pattern vector $v$ is the Hamming centroid of the rows in $P$.
\paragraph{neighbor} Similar to greedy graph growing, but less greedy, this one chooses a pivot row at random and initializes the pattern vector to the Hamming centroid of all the rows that share at least one non-zero location with the pivot row.

The above initialization heuristics are not claimed to be good in any sense; they just embody some principles and practices usually seen in Data Mining applications. A careful analysis of each method is beyond this paper. However, it is important to shed some intuition as to how these methods would perform in the $n \gg m$ setting found in dictionary learning problems. We will discuss this in the context of the proposed methods, which are described in the next section.

\subsection{Binary factorizations beyond rank one}

\def\xor{\oplus}

Let us now formalize the notation used for binary operations hereafter. We use $a \land b$ to denote the logical AND operation between binary operators $a$ and $b$; $a \lor b$ denotes logical OR, and $a \xor b$ the eXclusive OR (XOR); logical negation (NEG) is denoted by the unary operator, e.g., $\neg a$.
 
Now we return to our previous discussion. For rank one, it is easy to interpret the product $\hat{X}=uv\transp$; the element $\hat{x}_{ij}$ will be $1$ only $u_i$ and $v_j$ are $1$, that is, if we denote the AND operation by $\land$,  $\hat{x}_{ij} = u_i \odot v_j$. Furthermore, this product coincides with the \emph{standard} product for binary operators. If a rank-k, $k>1$ model is sought, even with binary factors, there are several ways in which the ranks can be combined to produce a result. One is to use modulo-2 arithmetic, that is, $\hat{X} = u_1v_1\transp \oplus u_2v_2\transp \oplus \ldots \oplus u_kv_k\transp$. Other is to use Boolean algebra, where addition is mapped to the OR operator, and we have $\hat{X} = u_1v_1\transp \lor u_2v_2\transp \lor \ldots \lor u_kv_k\transp$. Finally, we can use standard addition and then let the result be defined by some threshold, for example,
$\hat{X} = \indicator\left(\sum_{r=1}^k u_kv_k\transp \geq k/2 \right)$.
Which one is better will ultimately depend on the task and the prior information available about $X$.

\section{Binary Dictionary Learning}

This work proposes two methods for Binary Matrix Factorization which are essentially binary adaptations of the two Dictionary Learning methods discussed before, MOD and K-SVD. Correspondingly, we call these methods MOB (Method Of Binary directions) and K-PROX. As described in Section~\ref{sec:dictionary-learning}, the iterations of Dictionary Learning methods consist of an alternate minimization between the coefficients matrix $A$ and the dictionary matrix $D$. We now describe the coefficients update step, which is common to both methods.

\paragraph{Coefficients update via Binary Matching Pursuit (BMP)}
First of all, as $\ell_0$ norm and the $\ell_1$ norms are the same for binary vectors, we define a coefficients update step that is common to both dictionary learning methods, which will then differ only in the way that the dictionaries are updated.

We define BMP as a binarized variant of MP, that is, we begin with an zero coefficients vector $a=0$ and greedily incorporate, at iteration $k+1$, the atom that is closest to  $r\iter{k} = x - a\iter{k}$ in Hamming distance. (MP uses dot product, which is not a good idea in our case since the vectors are unnormalized). % hamming or Entropy?? 
The residual is correspondingly  updated by substracting the closest atom from it using modulo-2 algebra (XOR). The algorithm is formalized as follows:

\begin{algorithm}[ht]
\KwData{vector to encode $x$, dictionary $D$}
\KwResult{Optimal coefficients for $x$, $a$}
Set iteration $k=0$, residual $r\iter{0}=x$, coefficients $a\iter{0}=0$\;
\While{$h{r\iter{k}} \geq \epsilon$}{
  $h_{\min} \leftarrow \leftarrow \min h(D_i,r\iter{k}) $ \;
  \If{$h_{\min} > h(r\iter{k})$} { break }  
  $i \leftarrow \arg\min h(D_i,r\iter{k}) $ \;
  $a_i  \leftarrow  {a}_i  \oplus 1 $ \;
  $r\iter{k+1} \leftarrow r\iter{k} \oplus  D_i $\; 
  
}
\label{alg:bmp}
\end{algorithm}

Besides the differences mentioned, the algorithm also stops when the Hamming distance between the atom that is closest to the current residual is larger than the Hamming weight of the residual itself, as in such case there is no possible improvement. % show the number of maximum iterates! 

In the worst case, a maximum of $m$ steps will be required.
PENDING: PROVE THIS!

The main issue with this method, contrary to MP, is that there is no fast way to update the  
\subsection{MOB: Method Of Binary directions}
 
Here we want to update each atom so that the Hamming weight of the total residual matrix $E = X \oplus DA$ is minimum (note that minus and plus are the same thing in modulo-2 arithmetic). Say we want to update the $r$-th atom at iteration $k$. Clearly, the affected columns will only be those for which the  coefficients in $A$  corresponding to that atom are non-zero, that is $\{j : A_{rj} \neq 0 \}$; let us call this set $J_r$ and $n_r$ its size. What we want is the update $\Delta$ that, when added to $D_r$, minimizes the weight of the residual $E$ in those colums affected by $D_r$,
 \begin{eqnarray}
 \Delta  =& \arg\min_d \sum_{j \in J_r}  h(E\iter{k}_j \oplus d) \\
 =& \arg\min_d \sum_{j \in J_r} (\sum_i E\iter{k}_j + n_r d_i).
\label{eq:mob1}
 \end{eqnarray}

(note that the summation symbols are carried on using normal addition). The above objective is trivially separable in the elements of $d$,
 \begin{eqnarray}
 \Delta_i  =& \arg\min_{u \in \{0,1\}} \sum_j E_{ij}\iter{k} \oplus d_i\\
 =& \arg\min_{u \in \{0,1\}} \sum_{j\in J_r} E_{ij}\iter{k} + n_r u.
\label{eq:mob2}
 \end{eqnarray}
In other words, the $i$-th element of $D_r$ should be $0$ if most of the elements in the rows of $E$ affected by it are $0$, and $1$ otherwise. 
This is clearly an optimum solution (note that there could be many optima  when $n_r$ is even, in which case we simply choose one).

\subsection{K-PROX: Dictionary Update via Proximus}

In this case, following the K-SVD concept, we want to obtain the best rank-one approximation to the residual $E$ obtained after removing the contribution of $D_r$, 
\begin{equation}
(D_r,A_r) = \arg\min_{u,v} h(E \oplus uv\transp)
\label{eq:bsvd1}
\end{equation}
where
\begin{equation}
E =  X_{J_r} \oplus D\iter{k}A_{J_r}\iter{k} \oplus D_r\iter{k}(A_{J_r}\iter{k})^r,
\label{eq:bsvd2}
\end{equation}
where $X_{J_r}$ and $A_{J_r}$ contain only the colums in $J_r = \{j: A_{rj} = 1 \}$.

The problem  \refeq{eq:bsvd1} is, again, NP-hard, so an approximate solution must be sought. The idea here is to use alternate updates on $D_r$ and $A_r$, in which case the updates have a simple closed form solution. This is nothing buth the Proximus algorithm~\cite{proximus}. Say we want to minimize $h(E \oplus uv\transp)$ using alternate updates on $u$ and $v$. For fixed $u$, we have
$$
\sum_{i,j} E \oplus uv\transp = \sum_{j: v_j = 1} h(E_j \oplus u).
$$
We can solve this separately for each $v_j$ simply by setting $v_j = 1$ is $h(E_j \oplus u) < h(E_j)$ and $v_j=0$ otherwise. The update on $u$ is identical. In both cases, the objective function can only decrease, the function is bounded and the domain is compact, so that the algorithm must converge to a local minimum. Because the set is finite, convergence is attained in a finite number of iterations. Usually this number is quite small.

\subsection{Initialization}

First of all, if we are to apply these methods to the dictionary learning case, an important observation is that the \emph{pattern} vector $v$ is conceptually much closer to the concept of a dictionary atom, that is, a \emph{column} of $D$ in the decomposition $X=DA+E$, rather than a row of the coefficients matrix $A$. Therefore, when updating a given dictionary atom $D_k$ and its corresponding row of coefficients $A^k$ using proximus

The \emph{partition}...

SEGUIR


\subsection{Learning variants}

I implemented several variants of the above algorithms, most of them just change the order in which some updates are made, or switch the roles of dictionary and coefficients alternatively, so that the method can be used on a broader class of data.
 

\begin{enumerate}
\item Traditional: traditional alternate descent until local convergence
\item Role-switching I: at each iteration, the role of A and D are switched
\item Role-switched learning II: after local convergence (the same as in the first case), the role of A and D are switched and the traditional model is applied again
\item Role switched learning III: like type I but only the dictionary update step is applied (for use with Proximus)
\end{enumerate}

\section{Model selection}

The following two methods are actually meant to choose the best from an ensemble of candidate dictionaries learned using one of the previous four methods (referred to as ``inner learning method'' in the implementation). MDL stands for Minimum Description Length, and is a criterion for model selection (that is, choosing the best model out of a set of competing models for describing some particular data) where the criterion used is, essentially, ``which of the models produces a more succint description of the data''; in other words, which model compresses the most.
 
\begin{enumerate}
\item MDL/forward selection:  Here begin with an initial dictionary size $K_0$, train a dictionary, add a few more atoms to obtain $K_1$, and keep doing this until no further improvement is obtained, that is, until the overall codelength does not decrease by adding new atoms to the dictionary.
 
\item MDL/backward selection: We begin with a maximum dictionary size $K_0$, train the dictionary, and then remove the worst atom using some criterion (for example, remove one atom at a time and drop the one  which results in the largest decrease in the overall codelength). If removing an atom does not result in a better overall codelength, the method stops. 

\item MDL/full search: in both forward and backward selection, all or a part of the dictionary used in one iteration is kept and re-adapted after adding or removing atoms from it. In this case, for a range of values of $K$, a whole new dictionary is trained, and the best one is chosen among them. This method is much slower than the previous two ones (the fastest is forward selection), but tends to provide better results; This tradeoff is open for research.

\end{enumerate}


\section{Applications}

Here we describe some classical applications from the Dictionary Learning literature, with some references to some of the best results in each case.

\subsection{Denoising}

\paragraph{Main reference} \cite{ksvd}\\

Denoising is a special case of the more general \emph{signal restoration} problem (other examples are zooming, deblurring, or inpainting -- a.k.a. filling erased regions). Here we assume that we want to recover a signal sample $x \in \reals^{m}$ from a noisy observation $z \in \reals^{m}$, both related by
\[
z = x + n,
\]
where $n \in \reals^m$ is a vector of i.i.d. samples from some known distribution, e.g. Gaussian or Poisson. For instance, in the context of Image Processing, $z$ and $x$ are usually vectorized versions of square $\sqrt{m}\times\sqrt{m}$ patches of an image, and what one desires to recover is the whole image $I$ from its corresponding noisy version $J$.

The method used for denoising an image in the Dictionary Learning framework usually proceeds as follows:
\begin{itemize}
\item An initial dictionary $D_0 \in \reals^{m{\times}p}$ is available; this dictionary is learned to efficiently represent a \emph{large} set of patches taken from some public dataset of natural images, usually comprising thousands of images.
\item The image to be denoised, $J \in \reals^{M{\times}N}$, is decomposed into $n=(M-\sqrt{m}+1){\times}(N-\sqrt{m}+1)$ overlapping patches $(z_j : j = 1,\ldots,n)$ that we will arrange for convenience as columns in a matrix $Z \in \reals^{m{\times}n}$
\item The samples $Z$ are used to further adapt the initial dictionary and the accompanying sparse coefficients $A \in \reals^{p{\times}n}$; the sparse coding variant during the coefficient update step of this dictionary learning process is the \emph{denoising} formulation, given by
\[
a\iter{t+1}_j = \arg\min_a \|a\|_r \st \|z_j - D\iter{t}a \|_2 \leq C\sigma, 
\]
where $\sigma$ is the variance of the noise $C$ is a positive constant usually close to $1$ and $0 \leq r \leq 1$ is a sparsity-inducing (pseudo)-norm. The dictionary update is the same as described before in \refeq{nosedonde}
\item Upon convergence, the \emph{clean image patches $x_j$} are estimated from the solution $(D\opt,A\opt)$ as $$x_j = D\opt a\opt_j.$$
\item Finally, the estimated clean image is obtained by stitching back the estimated clean patches $x_j$ into their respective locations, averaging pixels where intersections between patches occur.
\end{itemize}

\subsection{Inpainting -- missing data}

Here the task is to fill in missing samples. As we only have binary matrices around, we need an auxiliary mask matrix $H \in \reals^{m{\times}n}$ which will tell us which samples are to be considered missing (value $0$) in the data matrix $X$, regardless of their value in $X$. The known values in $X$ are indicated with a $1$ in the corresponding place in $H$. The task is to fill in the missing values.

If we have a dictionary $D$ trained to samples similar to $X$, the idea is the following:

\begin{itemize}
\item For each sample $x$ with missing samples specified in a corresponding mask sample $h$,
\item Take the subset of rows from $D$ and $x$, $D_{h}$ and  $x_{h}$ for which the corresponding row in $h$ is $1$.
\item Encode $x_{h}$ using $D_{h}$ using the usual encoding scheme (that is, add atoms until no further decrease in the error is obtained); we obtain a vector of coefficients $a$
\item Fill in the subvector of missing values, $x_{\bar h}$ ($\bar h = 1 - h$) as, $$x_{\bar h} = D_{\bar h}a$$
\end{itemize}

\subsection{Classification}

Here again we discuss the case of image patches classification, including the special case where the patches are not part of a larger image but pre-cropped and aligned handwritten characters taken from the public datasets MNIST and USPS typically used in character recognition benchmarks.

There are several variants in this case. The one we'll mention here~\cite{ramirez10cvpr}, which seems quite natural, is a generative one where a dictionary $D_c$ is  adapted to efficiently represent samples from each class $c  \ in \mathcal{C}$. A new sample is then classified into class $c'$ if its representation under the dictionary $D_{c'}$ is better in some sense than that obtained using the dictionaries trained for the other classes. The method  can be summarized as follows:

\begin{itemize}
\item A  dictionary $D_c$ is adapted to a set of samples $X_c$ known to belong to class $c$ using some known method; this is done for each $c \in \mathcal{C}$. 
\item In order  to classifiy a new sample $x$, it is sparsely encoded using each of the dictionaries $D_c, c \in \mathcal{C}$, and the corresponding optimum coding cost is used as a score $l_c$. Any sparse coding variant could be used in principle, but typical choices here are the ``basis pursuit'' variant,
\[
l_c(x) = \min_a \|z_j - D\iter{t}a \|_2 \st  \|a\|_r \leq \tau, 
\]
and the Lagrangian variant,
\[
l_c(x) = \min_a \|z_j - D\iter{t}a \|_2 + \tau  \|a\|_r. 
\]
\item The sample $x$ is then declared to belong to the class $c$ so that  $l_c$ is the smallest. 
More so than in denoising, the success in this application depends largely  on the (critical) parameters $\tau$,  the $r$-norm  to be used, and the size of the dictionary $p$. These critical issues are treated in \cite{ramirez12tsp}.
\end{itemize}

\section{Results and discussion}

\subsection{Denoising}

FALTA

\subsection{Missing data}

FALTA

\subsection{Classification}

FALTA

\section{Conclusion}

FALTA

\bibliographystyle{plain}
\bibliography{bmf,sparse}

\end{document}
