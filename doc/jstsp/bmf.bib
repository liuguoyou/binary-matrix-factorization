@BOOK{mdl2,
  title = {Stochastic complexity in statistical inquiry},
  publisher = {Singapore: World Scientific},
  year = {1992},
  author = {J. Rissanen},
  owner = {nacho},
  timestamp = {2010.08.04}
}

@ARTICLE{mdl1,
  author = {J. Rissanen},
  title = {Universal coding, information,prediction, and estimation},
  journal = ieee_j_it,
  year = {1984},
  volume = {30},
  number = {4},
  pages={629--636},
  owner = {nacho},
  timestamp = {2010.08.04}
}

@ARTICLE{mdl3,
  author = {A. Barron and J. Rissanen and B. Yu},
  title = {The Minimum Description Length Principle in Coding and Modeling},
  journal = ieee_j_it,
  year = {1998},
  volume = {44},
  pages = {2743-2760},
  number = {6},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  owner = {nacho},
  timestamp = {2010.08.04}
}

@article{tropp07,
 author = {Tropp, J. A. and Gilbert, A. C.},
 title = {Signal Recovery From Random Measurements Via Orthogonal Matching Pursuit},
 journal = {IEEE Trans. Inf. Theor.},
 issue_date = {December 2007},
 volume = {53},
 number = {12},
 month = dec,
 year = {2007},
 issn = {0018-9448},
 pages = {4655--4666},
 numpages = {12},
 url = {http://dx.doi.org/10.1109/TIT.2007.909108},
 doi = {10.1109/TIT.2007.909108},
 acmid = {2273065},
 publisher = {IEEE Press},
 address = {Piscataway, NJ, USA},
 keywords = {Algorithms, approximation, basis pursuit, compressed sensing, group testing, orthogonal matching pursuit, signal recovery, sparse approximation},
}

@ARTICLE{dl-review,
author={Rubinstein, R. and Bruckstein, A. and Elad, M.},
journal={Proceedings of the IEEE}, 
title={Dictionaries for Sparse Representation Modeling},
year={2010},
month={June},
volume={98},
number={6},
pages={1045--1057},
keywords={dictionary learning;mathematical data model;redundant signal representation modeling;signal sampling;sparse signal representation modeling;training set;signal representation;signal sampling;wavelet transforms;},
doi={10.1109/JPROC.2010.2040551},
ISSN={0018-9219},
}

@ARTICLE{lewicki99,
  author = {M. Lewicki and B. Olshausen},
  title = {Probabilistic framework for the adaptation and comparison of image
	codes},
  journal = {J. Opt. Soc. Am. A},
  year = {1999},
  volume = {16},
  pages = {1587--1601},
  number = {7},
  keywords = {Probability theory, stochastic processes, and statistics; Image analysis;
	Image reconstruction techniques},
  owner = {nacho},
  publisher = {OSA},
  timestamp = {2010.08.04},
  url = {http://josaa.osa.org/abstract.cfm?URI=josaa-16-7-1587}
}

@ARTICLE{engan00,
  author = {K. Engan and S. Aase and J. Husoy},
  title = {Multi-frame compression: Theory and Design},
  journal = {Signal Processing},
  year = {2000},
  volume = {80},
  pages = {2121--2140},
  number = {10},
  month = {Oct.},
  bibsource = {http://www.visionbib.com/bibliography/image-proc175.html#TT13117},
  owner = {nacho},
  timestamp = {2010.08.04}
}


@ARTICLE{aharon06,
  author = {M. Aharon and M. Elad and A. Bruckstein},
  title = {The {K-SVD}: An Algorithm for Designing of Overcomplete Dictionaries
	for Sparse Representations},
  journal = ieee_j_sp,
  year = {2006},
  volume = {54},
  pages = {4311-4322},
  number = {11},
  month = {Nov.},
  owner = {nacho},
  timestamp = {2008.11.13}
}


@INPROCEEDINGS{proximus,
    author = {Mehmet Koyutürk and Ananth Grama},
    title = {{PROXIMUS}: A Framework for Analyzing Very High Dimensional Discrete-Attributed Datasets},
    booktitle = {{KDD} 2003},
    year = {2003},
    pages = {147--156},
    publisher = {IEEE Computer Society}
}

@ARTICLE{ksvd,
author={M. Aharon and M. Elad and A. Bruckstein},
journal={IEEE Transactions on Signal Processing},
title={\rm K -SVD: An Algorithm for Designing Overcomplete Dictionaries for Sparse Representation},
year={2006},
volume={54},
number={11},
pages={4311-4322},
keywords={image coding;image representation;iterative methods;singular value decomposition;transforms;K-SVD;K-means clustering process;image data;iterative method;linear transforms;overcomplete dictionary;signals sparse representation;sparse coding;sparsity constraints;Algorithm design and analysis;Clustering algorithms;Dictionaries;Feature extraction;Inverse problems;Iterative algorithms;Matching pursuit algorithms;Prototypes;Pursuit algorithms;Signal design;Atom decomposition;FOCUSS;basis pursuit;codebook;dictionary;gain-shape VQ;matching pursuit;sparse representation;training;vector quantization},
doi={10.1109/TSP.2006.881199},
ISSN={1053-587X},
month={Nov},}


@article{bam,
 author = {Kosko, Bart},
 title = {Bidirectional Associative Memories},
 journal = {IEEE Trans. Syst. Man Cybern.},
 issue_date = {January/February 1988},
 volume = {18},
 number = {1},
 month = jan,
 year = {1988},
 issn = {0018-9472},
 pages = {49--60},
 numpages = {12},
 url = {http://dx.doi.org/10.1109/21.87054},
 doi = {10.1109/21.87054},
 acmid = {46936},
 publisher = {IEEE Press},
 address = {Piscataway, NJ, USA},
} 

@ARTICLE{mod,
  author = {K. Engan and S. Aase and J. Husoy},
  title = {Multi-frame compression: Theory and Design},
  journal = {Signal Processing},
  year = {2000},
  volume = {80},
  pages = {2121--2140},
  number = {10},
  month = {Oct.},
  bibsource = {http://www.visionbib.com/bibliography/image-proc175.html#TT13117},
  owner = {nacho},
  timestamp = {2010.08.04}
}

@ARTICLE{lasso,
  author = {R. Tibshirani},
  title = {Regression Shrinkage and Selection via the {LASSO}},
  journal = {Journal of the Royal Statistical Society: Series B},
  year = {1996},
  volume = {58},
  pages = {267-288},
  number = {1},
  owner = {nacho},
  series = {B (Methodological)},
  timestamp = {2010.08.04}
}

@ARTICLE{mp,
  author = {S. Mallat and Z. Zhang},
  title = {Matching Pursuit in a time-frequency dictionary},
  journal = IEEE_J_SP,
  year = {1993},
  volume = {41},
  pages = {3397-3415},
  number = {12}
}

@article{omp,
author = {Davis, G and Mallat, St{\'{e}}phane and Avellaneda, Marco},
journal = {Constr. Approx},
pages = {57--98},
title = {{Greedy adaptive approximation}},
volume = {13},
year = {1997}
}

@article{pca,
  added-at = {2009-04-04T18:01:35.000+0200},
  author = {Pearson, K.},
  biburl = {https://www.bibsonomy.org/bibtex/2e3cae747969ec5a442f434a00d9d4f61/dieudonnew},
  interhash = {c2a88da8d5068eb09941df30fa3b0f23},
  intrahash = {e3cae747969ec5a442f434a00d9d4f61},
  issue = {6},
  journal = {Philosophical Magazine},
  keywords = {imported},
  pages = {559-572},
  timestamp = {2009-04-04T18:01:35.000+0200},
  title = {On Lines and Planes of Closest Fit to Systems of Points in Space},
  volume = 2,
  year = 1901
}

@article{rpca,
author = {A. Campbell, N},
year = {1980},
month = {01},
pages = {},
title = {Robust Procedures in Multivariate Analysis I: Robust Covariance Estimation},
volume = {29},
booktitle = {Applied Statistics}
}

@article{rpca-book,
author={R. Vidal and Y. Ma and S. Sastry},
 year={2016},
 title={Generalized Principal Component Analysus}, 
 publisher={Springer} 
}
 
@ARTICLE{rpca-paper, 
author={R. Vidal and Yi Ma and S. Sastry}, 
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
title={Generalized principal component analysis (GPCA)}, 
year={2005}, 
volume={27}, 
number={12}, 
pages={1945-1959}, 
keywords={computer vision;expectation-maximisation algorithm;geometry;polynomials;principal component analysis;algebro-geometric solution;expectation maximization;generalized principal component analysis;homogeneous polynomials;iterative techniques;polynomial factorization;subspace segmentation;Application software;Clustering algorithms;Computer vision;Iterative algorithms;Kernel;Machine learning;Matrix decomposition;Motion segmentation;Polynomials;Principal component analysis;Index Terms- Principal component analysis (PCA);Veronese map;dimensionality reduction;dynamic scenes and motion segmentation.;subspace segmentation;temporal video segmentation;Algorithms;Artificial Intelligence;Cluster Analysis;Computer Simulation;Image Enhancement;Image Interpretation, Computer-Assisted;Imaging, Three-Dimensional;Information Storage and Retrieval;Models, Statistical;Pattern Recognition, Automated;Principal Component Analysis}, 
doi={10.1109/TPAMI.2005.244}, 
ISSN={0162-8828}, 
month={Dec},}
 
@ARTICLE{nmf-review,
   author = {{Gillis}, N.},
    title = "{Introduction to Nonnegative Matrix Factorization}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1703.00663},
 primaryClass = "cs.NA",
 keywords = {Computer Science - Numerical Analysis, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning, Mathematics - Optimization and Control, Statistics - Machine Learning},
     year = 2017,
    month = mar,
   adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170300663G},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{monson95,
 author={S.~D. Monson and N. J. Pullman and R. Rees},
 title={A survey of clique and biclique coverings and factorizations of (0, 1)-matrices},
 journal={Bull. Inst. Combin. Appl.}, 
 volume={14},
 pages={17--86}, 
 year={1995}
}

@article{tiling,
  author={F. Geerts and B. Goethals and T. Mielik\"{a}inen},
  title={Tiling Databases},
  journal={Discovery Science},
  volume={3245}, 
  pages={278--289}, 
  publisher={Springer}, 
  year={2004}
}

@phdthesis{asso,
  title={Matrix Decomposition Methods for Data Mining: Computational Complexity and Algorithms},
  author={P. Miettinen},
  year={2009},
  school={University of Helsinki}
}

@article{bmf-dm-naquever,
title = "On the Positive–Negative Partial Set Cover problem",
journal = "Information Processing Letters",
volume = "108",
number = "4",
pages = "219 - 221",
year = "2008",
issn = "0020-0190",
doi = "https://doi.org/10.1016/j.ipl.2008.05.007",
url = "http://www.sciencedirect.com/science/article/pii/S0020019008001592",
author = "Pauli Miettinen",
keywords = "Approximation algorithms, Combinatorial problems, Hardness of approximation, Set cover",
abstract = "The Positive–Negative Partial Set Cover problem is introduced and its complexity, especially the hardness-of-approximation, is studied. The problem generalizes the Set Cover problem, and it naturally arises in certain data mining applications."
}
@INPROCEEDINGS{bmf07, 
author={Z. Zhang and T. Li and C. Ding and X. Zhang}, 
booktitle={Seventh IEEE International Conference on Data Mining (ICDM 2007)}, 
title={Binary Matrix Factorization with Applications}, 
year={2007}, 
volume={}, 
number={}, 
pages={391-400}, 
abstract={An interesting problem in nonnegative matrix factorization (NMF) is to factorize the matrix X which is of some specific class, for example, binary matrix. In this paper, we extend the standard NMF to binary matrix factorization (BMF for short): given a binary matrix X, we want to factorize X into two binary matrices W, H (thus conserving the most important integer property of the objective matrix X) satisfying X ap WH. Two algorithms are studied and compared. These methods rely on a fundamental boundedness property of NMF which we propose and prove. This new property also provides a natural normalization scheme that eliminates the bias of factor matrices. Experiments on both synthetic and real world datasets are conducted to show the competency and effectiveness of BMF.}, 
keywords={matrix decomposition;bias elimination;binary matrix factorization;fundamental boundedness property;integer property;natural normalization scheme;nonnegative matrix factorization;Application software;Clustering algorithms;Computer science;DNA;Data analysis;Data mining;Machine learning;Matrix decomposition;Proteins;USA Councils}, 
doi={10.1109/ICDM.2007.99}, 
ISSN={1550-4786}, 
month={Oct},}

@inproceedings{bmf-mp,
 author = {Ravanbakhsh, Siamak and P\'{o}czos, Barnab\'{a}s and Greiner, Russell},
 title = {Boolean Matrix Factorization and Noisy Completion via Message Passing},
 booktitle = {Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48},
 series = {ICML'16},
 year = {2016},
 location = {New York, NY, USA},
 pages = {945--954},
 numpages = {10},
 url = {http://dl.acm.org/citation.cfm?id=3045390.3045491},
 acmid = {3045491},
 publisher = {JMLR.org},
}

@inproceedings{bmf13,
 author = {Slawski, Martin and Hein, Matthias and Lutsik, Pavlo},
 title = {Matrix Factorization with Binary Components},
 booktitle = {Proceedings of the 26th International Conference on Neural Information Processing Systems - Volume 2},
 series = {NIPS'13},
 year = {2013},
 location = {Lake Tahoe, Nevada},
 pages = {3210--3218},
 numpages = {9},
 url = {http://dl.acm.org/citation.cfm?id=2999792.2999970},
 acmid = {2999970},
 publisher = {Curran Associates Inc.},
 address = {USA},
}


@article{bmf06,
author = {Miettinen, Pauli and Mielikäinen, Taneli and Gionis, Aristides and Das, Gautam and Mannila, Heikki},
year = {2006},
month = {09},
pages = {335-346},
title = {The Discrete Basis Problem},
volume = {20},
journal = {Knowledge and Data Engineering, IEEE Transactions on}
}

@inproceedings{bmf-mdl,
 author = {Miettinen, Pauli and Vreeken, Jilles},
 title = {Model Order Selection for Boolean Matrix Factorization},
 booktitle = {Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
 series = {KDD '11},
 year = {2011},
 isbn = {978-1-4503-0813-7},
 location = {San Diego, California, USA},
 pages = {51--59},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/2020408.2020424},
 doi = {10.1145/2020408.2020424},
 acmid = {2020424},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {boolean matrix factorizations, matrix decompositions, matrix factorizations, minimum description length principle, model order selection, model selection},
}

@Article{amp,
author={Donoho, David L.
and Maleki, Arian
and Montanari, Andrea},
title={Message-passing algorithms for compressed sensing},
journal={Proc Natl Acad Sci U S A},
year={2009},
month={Nov},
day={10},
publisher={National Academy of Sciences},
volume={106},
number={45},
pages={18914-18919},
abstract={Compressed sensing aims to undersample certain high-dimensional signals yet accurately reconstruct them by exploiting signal characteristics. Accurate reconstruction is possible when the object to be recovered is sufficiently sparse in a known basis. Currently, the best known sparsity-undersampling tradeoff is achieved when reconstructing by convex optimization, which is expensive in important large-scale applications. Fast iterative thresholding algorithms have been intensively studied as alternatives to convex optimization for large-scale problems. Unfortunately known fast algorithms offer substantially worse sparsity-undersampling tradeoffs than convex optimization. We introduce a simple costless modification to iterative thresholding making the sparsity-undersampling tradeoff of the new algorithms equivalent to that of the corresponding convex optimization procedures. The new iterative-thresholding algorithms are inspired by belief propagation in graphical models. Our empirical measurements of the sparsity-undersampling tradeoff for the new algorithms agree with theoretical calculations. We show that a state evolution formalism correctly derives the true sparsity-undersampling tradeoff. There is a surprising agreement between earlier calculations based on random convex polytopes and this apparently very different theoretical formalism.},
note={0069[PII]},
note={19858495[pmid]},
issn={0027-8424},
doi={10.1073/pnas.0909892106},
url={http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2767368/}
}

@INPROCEEDINGS{gamp, 
author={S. Rangan}, 
booktitle={2011 IEEE International Symposium on Information Theory Proceedings}, 
title={Generalized approximate message passing for estimation with random linear mixing}, 
year={2011}, 
volume={}, 
number={}, 
pages={2168-2172}, 
abstract={We consider the estimation of a random vector observed through a linear transform followed by a componentwise probabilistic measurement channel. Although such linear mixing estimation problems are generally highly non-convex, Gaussian approximations of belief propagation (BP) have proven to be computationally attractive and highly effective in a range of applications. Recently, Bayati and Montanari have provided a rigorous and extremely general analysis of a large class of approximate message passing (AMP) algorithms that includes many Gaussian approximate BP methods. This paper extends their analysis to a larger class of algorithms to include what we call generalized AMP (G-AMP). G-AMP incorporates general (possibly non-AWGN) measurement channels. Similar to the AWGN output channel case, we show that the asymptotic behavior of the G-AMP algorithm under large i.i.d. Gaussian transform matrices is described by a simple set of state evolution (SE) equations. The general SE equations recover and extend several earlier results, including SE equations for approximate BP on general output channels by Guo and Wang.}, 
keywords={Gaussian channels;matrix algebra;message passing;transforms;vectors;Gaussian approximations;Gaussian transform matrices;belief propagation;component-wise probabilistic measurement channel;general output channels;generalized AMP;generalized approximate message passing;linear transform;random linear mixing;random vector;state evolution equations;Algorithm design and analysis;Approximation algorithms;Approximation methods;Belief propagation;Equations;Estimation;Mathematical model;Optimization;belief propagation;compressed sensing;estimation;random matrices}, 
doi={10.1109/ISIT.2011.6033942}, 
ISSN={2157-8095}, 
month={July},}


@CONFERENCE{bmf-nmf2,
author={Diop, M. and Larue, A. and Miron, S. and Brie, D.},
title={A post-nonlinear mixture model approach to binary matrix factorization},
journal={25th European Signal Processing Conference, EUSIPCO 2017},
year={2017},
volume={2017-January},
pages={321-325},
doi={10.23919/EUSIPCO.2017.8081221},
art_number={8081221},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041507709&doi=10.23919%2fEUSIPCO.2017.8081221&partnerID=40&md5=d41896dc130fdef8ef4fab5df809290e},
abstract={In this paper, we address the Binary Matrix Factorization (BMF) problem which is the restriction of the nonnegative matrix factorization (NMF) to the binary matrix case. A necessary and sufficient condition for the identifiability for the BFM model is given. We propose to approach the BMF problem by the NMF problem using a nonlinear function which guarantees the binarity of the reconstructed data. Two new algorithms are introduced and compared in simulations with the state of art BMF algorithms. © EURASIP 2017.},
publisher={Institute of Electrical and Electronics Engineers Inc.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{bmf-tiling-mdl,
author={Hess, S. and Morik, K. and Piatkowski, N.},
title={The PRIMPING routine—Tiling through proximal alternating linearized minimization},
journal={Data Mining and Knowledge Discovery},
year={2017},
volume={31},
number={4},
pages={1090-1131},
doi={10.1007/s10618-017-0508-z},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019214349&doi=10.1007%2fs10618-017-0508-z&partnerID=40&md5=1ffa8a9d123389f018e01701ade191c5},
abstract={Mining and exploring databases should provide users with knowledge and new insights. Tiles of data strive to unveil true underlying structure and distinguish valuable information from various kinds of noise. We propose a novel Boolean matrix factorization algorithm to solve the tiling problem, based on recent results from optimization theory. In contrast to existing work, the new algorithm minimizes the description length of the resulting factorization. This approach is well known for model selection and data compression, but not for finding suitable factorizations via numerical optimization. We demonstrate the superior robustness of the new approach in the presence of several kinds of noise and types of underlying structure. Moreover, our general framework can work with any cost measure having a suitable real-valued relaxation. Thereby, no convexity assumptions have to be met. The experimental results on synthetic data and image data show that the new method identifies interpretable patterns which explain the data almost always better than the competing algorithms. © 2017, The Author(s).},
author_keywords={Alternating minimization;  Boolean matrix factorization;  Minimum description length principle;  Nonconvex-nonsmooth minimization;  Proximal alternating linearized minimization;  Tiling},
publisher={Springer New York LLC},
document_type={Article},
source={Scopus},
}


@ARTICLE{ternary2,
author={Maurus, S. and Plant, C.},
title={Ternary Matrix Factorization: problem definitions and algorithms},
journal={Knowledge and Information Systems},
year={2016},
volume={46},
number={1},
pages={1-31},
doi={10.1007/s10115-015-0838-3},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84952714500&doi=10.1007%2fs10115-015-0838-3&partnerID=40&md5=6d1c5acef2bef107743957195961f347},
abstract={Can we learn from the unknown? Logical data sets of the ternary kind are often found in information systems. They contain unknown as well as true/false values. An unknown value may represent a missing entry (lost or indeterminable) or have meaning, like a Don’t Know response in a questionnaire. In this paper, we introduce algorithms for reducing the dimensionality of logical data (categorical data in general) in the context of a new data mining challenge: Ternary Matrix Factorization (TMF). For a ternary data matrix, TMF exploits ternary logic to produce a basis matrix (which holds the major patterns in the data) and a usage matrix (which maps patterns to original observations). Both matrices are interpretable, and their ternary matrix product approximates the original matrix. TMF has applications in (1) finding targeted structure in ternary data, (2) imputing values through pattern discovery in highly incomplete categorical data sets, and (3) solving instances of its encapsulated Binary Matrix Factorization problem. Our elegant algorithm FasTer (FASt TERnary Matrix Factorization) has linear run-time complexity with respect to the dimensions of the data set and is parameter-robust. A variant of FasTer that exploits useful results from combinatorics provides accuracy bounds for a core part of the algorithm in certain situations. Experiments on synthetic and real-world data sets show that our algorithms are able to outperform state-of-the-art techniques in all three TMF applications with respect to run-time and effectiveness. Finally, convincing speedup and efficiency results on a parallel version of FasTer demonstrate its suitability for weak- and strong-scaling scenarios. © 2015, Springer-Verlag London.},
author_keywords={Dimensionality reduction;  Imputation;  Matrix factorization;  Missing values;  Parallel algorithms;  Ternary data;  Three-valued logic},
publisher={Springer-Verlag London Ltd},
document_type={Article},
source={Scopus},
}

@CONFERENCE{bmf-qp,
author={Hamid, S. and Eric, M. and Termier, G.A.},
title={Improved local search for binary matrix factorization},
journal={Proceedings of the National Conference on Artificial Intelligence},
year={2015},
volume={2},
pages={1198-1204},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959913421&partnerID=40&md5=b3976300ca8e93a366968ec3ac14d747},
abstract={Rank K Binary Matrix Factorization (BMF) approximates a binary matrix by the product of two binary matrices of lower rank, K, using either L1 or L2 norm. In this paper, we first show that the BMF with L2 norm can be reformulated as an Unconstrained Binary Quadratic Programming (UBQP) problem. We then review several local search strategies that can be used to improve the BMF solutions obtained by previously proposed methods, before introducing a new local search dedicated to the BMF problem. We show in particular that the proposed solution is in general faster than the previously proposed ones. We then assess its behavior on several collections and methods and show that it significantly improves methods targeting the L2 norms on all the datasets considered; for the L1 norm, the improvement is also significant for real, structured datasets and for the BMF problem without the binary reconstruction constraint. Copyright © 2015, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.},
publisher={AI Access Foundation},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{bah1,
author={Mireles, V. and Conrad, T.O.F.},
title={Minimum-overlap clusterings and the sparsity of overcomplete decompositions of binary matrices.},
journal={Procedia Computer Science},
year={2015},
volume={51},
pages={2967-2971},
doi={10.1016/j.procs.2015.05.500},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84939488326&doi=10.1016%2fj.procs.2015.05.500&partnerID=40&md5=308a8c9c436782f9625eebf26768cadb},
abstract={Given a set of n binary data points, a widely used technique is to group its features into k clusters (e.g.[7]). In the case where n < k, the question of how overlapping are the clusters becomes of interest. In this paper we approach the question through matrix decomposition, and relate the degree of overlap with the sparsity of one of the resulting matrices. We present analytical results regarding bounds on this sparsity, and a heuristic to estimate the minimum amount of overlap that an exact grouping of features into k clusters must have. As shown below, adding new data will not alter this minimum amount of overlap. © The Authors. Published by Elsevier B.V.},
author_keywords={Binary matrix factorization;  Feature clustering;  Overcomplete decompositions},
publisher={Elsevier B.V.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{bmf-comp,
author={Bartl, E. and Belohlavek, R. and Osicka, P. and Řezanková, H.},
title={Dimensionality reduction in boolean data: Comparison of four BMF methods},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2015},
volume={7627},
pages={118-133},
doi={10.1007/978-3-662-48577-4_8},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951873972&doi=10.1007%2f978-3-662-48577-4_8&partnerID=40&md5=e50e98eac25837713668e5006fed8dd1},
abstract={We compare four methods for Boolean matrix factorization (BMF). The oldest of these methods is the 8M method implemented in the BMDP statistical software package developed in the 1960s. The three other methods were developed recently. All the methods compute from an input object-attribute matrix I two matrices, namely an object-factor matrix A and a factor-attribute matrix B in such a way that the Boolean matrix product of A and B is approximately equal to I. Such decompositions are utilized directly in Boolean factor analysis or indirectly as a dimensionality reduction method for Boolean data in machine learning. While some comparison of the BMF methods with matrix decomposition methods designed for real valued data exists in the literature, a mutual comparison of the various BMF methods is a severely neglected topic. In this paper, we compare the four methods on real datasets. In particular, we observe the reconstruction ability of the first few computed factors as well as the number of computed factors necessary to fully reconstruct the input matrix, i.e. the approximation to the Boolean rank of I computed by the methods. In addition, we present some general remarks on all the methods being compared. © Springer-Verlag Berlin Heidelberg 2015.},
publisher={Springer Verlag},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{bmf-logistic-nmf,
author={Larsen, J.S. and Clemmensen, L.K.H.},
title={Non-negative matrix factorization for binary data},
journal={IC3K 2015 - Proceedings of the 7th International Joint Conference on Knowledge Discovery, Knowledge Engineering and Knowledge Management},
year={2015},
volume={1},
pages={555-563},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960877335&partnerID=40&md5=a0e5fcbf9cb63680cc02da3a28b629d2},
abstract={We propose the Logistic Non-negative Matrix Factorization for decomposition of binary data. Binary data are frequently generated in e.g. text analysis, sensory data, market basket data etc. A common method for analysing non-negative data is the Non-negative Matrix Factorization, though this is in theory not appropriate for binary data, and thus we propose a novel Non-negative Matrix Factorization based on the logistic link function. Furthermore we generalize the method to handle missing data. The formulation of the method is compared to a previously proposed logistic matrix factorization without non-negativity constraint on the features. We compare the performance of the Logistic Non-negative Matrix Factorization to Least Squares Non-negative Matrix Factorization and Kullback-Leibler (KL) Non-negative Matrix Factorization on sets of binary data: a synthetic dataset, a set of student comments on their professors collected in a binary termdocument matrix and a sensory dataset. We find that choosing the number of components is an essential part in the modelling and interpretation, that is still unresolved. © 2015 by SCITEPRESS - Science and Technology Publications, Lda.},
author_keywords={Binary data;  Binary matrix factorization;  Non-negative matrix factorization;  Text modelling},
publisher={SciTePress},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{ternary,
author={Maurus, S. and Plant, C.},
title={Ternary Matrix Factorization},
journal={Proceedings - IEEE International Conference on Data Mining, ICDM},
year={2015},
volume={2015-January},
number={January},
pages={400-409},
doi={10.1109/ICDM.2014.40},
art_number={7023357},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84936980906&doi=10.1109%2fICDM.2014.40&partnerID=40&md5=0cb9987c9802aad9a47cf3742c43c7be},
abstract={Can we learn from the unknown? Logical data sets of the ternary kind are often found in information systems. They contain unknown as well as true/false values. An unknown value may represent a missing entry (lost or indeterminable) or something with meaning, like a 'Don't Know' response in a questionnaire. In this paper we introduce an effectively- and efficiently-superior algorithm for reducing the dimensionality of logical data (categorical data in general) in the context of a new data mining challenge: Ternary Matrix Factorization (TMF). For a ternary data matrix, TMF exploits ternary logic to produce a basis matrix (which holds the major patterns in the data) and a usage matrix (which maps patterns to original observations). Both matrices are interpretable, and their ternary matrix product approximates the original matrix. TMF has applications in 1) finding targeted structure in ternary data, 2) imputing values through pattern-discovery in highly-incomplete categorical data sets, and 3) solving instances of its encapsulated Binary Matrix Factorization (BMF) problem. Our elegant algorithm Faster (Fast Ternary Matrix Factorization) has linear run-time complexity with respect to the dimensions of the data set and is parameter-robust. Experiments on synthetic and real-world data sets show that we are able to efficiently and effectively outperform state-of-the-art techniques in all three TMF applications. © 2014 IEEE.},
author_keywords={dimensionality reduction;  imputation;  matrix factorization;  missing values;  ternary data;  Three-valued logic},
publisher={Institute of Electrical and Electronics Engineers Inc.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{bmf-sel,
author={Liu, C. and Feng, L. and Fujimaki, R. and Muraoka, Y.},
title={Scalable model selection for large-scale factorial relational models},
journal={32nd International Conference on Machine Learning, ICML 2015},
year={2015},
volume={2},
pages={1227-1235},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969787296&partnerID=40&md5=f28dc3c858eda2537da527b94e1ad982},
abstract={With a growing need to understand large-scale networks, factorial relational models, such as binary matrix factorization models (BMFs), have become important in many applications. Although BMFs have a natural capability to uncover overlapping group structures behind network data, existing inference techniques have issues of either high computational cost or lack of model selection capability, and this limits their applicability. For scalable model selection of BMFs, this paper proposes stochastic factorized asymptotic Bayesian (sFAB) inference that combines concepts in two recently-developed techniques: stochastic variational inference (SVI) and FAB inference. sFAB is a highly-efficient algorithm, having both scalability and an inherent model selection capability in a single inference framework. Empirical results show the superiority of sFAB/BMF in both accuracy and scalability over state-of-the-art inference methods for overlapping relational models. Copyright © 2015 by the author(s).},
publisher={International Machine Learning Society (IMLS)},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{bmf-em,
author={Frolov, A.A. and Husek, D. and Polyakov, P.Y.},
title={Two expectation-maximization algorithms for boolean factor analysis},
journal={Neurocomputing},
year={2014},
volume={130},
pages={83-97},
doi={10.1016/j.neucom.2012.02.055},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893735667&doi=10.1016%2fj.neucom.2012.02.055&partnerID=40&md5=4fdf7ebe4df2dca3f9b1f7c65e89dd7c},
abstract={Methods for the discovery of hidden structures of high-dimensional binary data are one of the most important challenges facing the community of machine learning researchers. There are many approaches in the literature that try to solve this hitherto rather ill-defined task. In the present study, we propose a general generative model of binary data for Boolean Factor Analysis and introduce two new Expectation-Maximization Boolean Factor Analysis algorithms which maximize the likelihood of a Boolean Factor Analysis solution. To show the maturity of our solutions we propose an informational measure of Boolean Factor Analysis efficiency. Using the so-called bars problem benchmark, we compare the efficiencies of the proposed algorithms to that of Dendritic Inhibition Neural Network, Maximal Causes Analysis, and Boolean Matrix Factorization. Last mentioned methods were taken as related methods as they are supposed to be the most efficient in bars problem benchmark. Then we discuss the peculiarities of the two methods we proposed and the three related methods in performing Boolean Factor Analysis. © 2013 Elsevier B.V.},
author_keywords={Bars problem;  Binary data model;  Binary Matrix Factorization;  Boolean Factor Analysis;  Dimension reduction;  Neural networks},
document_type={Article},
source={Scopus},
}

@CONFERENCE{rank1,
author={Shi, Z. and Wang, L. and Shi, L.},
title={Approximation method to rank-one binary matrix factorization},
journal={IEEE International Conference on Automation Science and Engineering},
year={2014},
volume={2014-January},
pages={800-805},
doi={10.1109/CoASE.2014.6899417},
art_number={6899417},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84940189426&doi=10.1109%2fCoASE.2014.6899417&partnerID=40&md5=807b71b0e80cd3f24590f9521923b875},
abstract={In this paper, we study the rank-one binary matrix factorization problem, which plays a key role in analyzing the high dimensional discrete-attributed datasets. We first transform this problem into an equivalent binary quadratic programming, and then a mixed-integer linear formulation is obtained using linearization technique. A characterization on the extreme points is given for the polyhedron of the linearized formulation. To the best of our knowledge, this is the first time to study the extreme points property of the rank-one binary matrix factorization. Based on this property, a LP rounding based 2-approximation algorithm is designed for this problem. Numerical results illustrate the efficiency of the proposed approximation algorithm. © 2014 IEEE.},
publisher={IEEE Computer Society},
document_type={Conference Paper},
source={Scopus},
}

@Book{bmf-oldest,
title = {BMDP Statistical Software Manual},
author = {M.R. Mickey and P. Mundle and L. Engelman},
publisher = {University of California Press},
year = {1990},
volume= {2},
pages = {849--860},
}

@inproceedings{online-dl,
  author={J. Mairal and F. Bach and J. Ponce and G. Sapiro},
  title={Online Dictionary Learning for Sparse Coding},
  booktitle={ICML 2009, Montreal, Canada}, 
  year={2009}
}

@article{matrix-inv-lemma,
  author = {William W. Hager},
  title = {Updating the Inverse of a Matrix},
  journal = {SIAM Review},
  volume = {31},
  number = {2},
  pages = {221-239},
  year = {1989},
  doi = {10.1137/1031049},
  URL = {https://doi.org/10.1137/1031049},
  eprint = {https://doi.org/10.1137/1031049}
}

@article{fista,
  author = {Amir Beck and Marc Teboulle},
  title = {A Fast Iterative Shrinkage-Thresholding Algorithm for Linear   Inverse Problems},
  journal = {SIAM Journal on Imaging Sciences},
  volume = {2},
  number = {1},
  pages = {183-202},
  year = {2009},
  doi = {10.1137/080716542},
  URL = {https://doi.org/10.1137/080716542},
  eprint = {https://doi.org/10.1137/080716542}
}

@inproceedings{association-accuracy,
 author = {Agrawal, Rakesh and Imieli\'{n}ski, Tomasz and Swami, Arun},
 title = {Mining Association Rules Between Sets of Items in Large Databases},
 booktitle = {Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data},
 series = {SIGMOD '93},
 year = {1993},
 isbn = {0-89791-592-5},
 location = {Washington, D.C., USA},
 pages = {207--216},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/170035.170072},
 doi = {10.1145/170035.170072},
 acmid = {170072},
 publisher = {ACM},
 address = {New York, NY, USA},
}

@ARTICLE{bic,
  author = {G. Schwartz},
  title = {Estimating the dimension of a model},
  journal = annals_stat,
  year = {1978},
  volume = {6},
  pages = {461--464},
  number = {2},
  owner = {nacho},
  timestamp = {2010.08.04}
}

@ARTICLE{aic,
   author = {H. Akaike},
    title = {A New Look at the Statistical Model Identification},
  journal = {IEEE Transactions on Automatic Control},
 keywords = {MAXIMUM ENTROPY, POWER SPECTRA},
     year = 1974,
   volume = 19,
    pages = {716-723},
   adsurl = {http://adsabs.harvard.edu/abs/1974ITAC...19..716A},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{enum, 
author={T. Cover}, 
journal={IEEE Transactions on Information Theory}, 
title={Enumerative source encoding}, 
year={1973}, 
volume={19}, 
number={1}, 
pages={73-77}, 
abstract={LetSbe a given subset of binary n-sequences. We provide an explicit scheme for calculating the index of any sequence inSaccording to its position in the lexicographic ordering ofS. A simple inverse algorithm is also given. Particularly nice formulas arise whenSis the set of alln-sequences of weightkand also whenSis the set of all sequences having a given empirical Markov property. Schalkwijk and Lynch have investigated the former case. The envisioned use of this indexing scheme is to transmit or store the index rather than the sequence, thus resulting in a data compression of(logmidSmid)/n.}, 
keywords={Sequences;Source coding}, 
doi={10.1109/TIT.1973.1054929}, 
ISSN={0018-9448}, 
month={January},}


@Misc{mnist,
  author = {Y. Lecun},
  title={The {MNIST} database of handwritten digits},
  howpublished={http://yann.lecun.com/exdb/mnist/},
}

@inproceedings{usps,
title = "Handwritten character recognition using neural network architectures",
author = "Ofer Matan and R.K. Kiang and Stenard, {C. E.} and B. Boser and Denker, {J. S.} and D. Henderson and Howard, {R. E.} and W. Hubbard and L.D. Jackel and Yann Lecun",
year = "1990",
language = "English (US)",
booktitle = "Proceedings of the 4th US Postal Service Advanced Technology Conference, Washington D.C., November 1990",
}